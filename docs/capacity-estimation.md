üìä Capacity Estimation & Scaling Strategy ‚Äî Cricket Live Platform
1. Purpose

This document estimates traffic, storage, memory, and compute needs for the Cricket Live Platform and defines a scaling strategy to handle peak loads during live matches.

2. Assumptions
Traffic Assumptions (Peak Event)

Concurrent live matches: 50

Average concurrent users per match: 20,000

Peak concurrent users total: 1,000,000

Admin scorers per match: 2

Match duration: 3 hours

3. Read & Write Patterns
3.1 Write Traffic (Admin Scoring)

Ball updates per over: 6

Overs per match: 50

Total balls per match: 300

Events per match: ~350 (including wickets, extras)

Writes per second (peak):

50 matches √ó 1 ball / 30 sec ‚âà 1.6 writes/sec


‚û°Ô∏è Very low write traffic (easy to scale)

3.2 Read Traffic (Users)
REST API Reads

Initial match load per user

Assume 1 REST fetch per user

1,000,000 users / 10 minutes ‚âà 1,666 RPS

WebSocket Messages

Every ball update broadcasted

50 matches √ó 1 update / 30 sec = ~1.6 events/sec


Each event fan-outs to:

20,000 users per match


‚û°Ô∏è WebSocket fan-out is the real load, not REST.

4. WebSocket Capacity
Concurrent Connections

Peak WebSocket connections: 1M

Backend Instance Capacity

Conservative estimate: 50,000 connections / instance

1,000,000 / 50,000 = 20 instances


‚û°Ô∏è Plan for 25 instances (buffer)

5. Redis Capacity Estimation
5.1 Key Count
Key Type	Per Match	Total
Live state	1	50
Event list	1	50
Subscribers	1	50
Rate limits	~10K	~500K
5.2 Memory Usage
Live State

~1 KB per match ‚Üí 50 KB

Events

100 events √ó 300 bytes = 30 KB / match ‚Üí 1.5 MB

Rate Limits

~100 bytes/key √ó 500K ‚Üí 50 MB

‚û°Ô∏è Total Redis Memory ‚âà 100 MB

Provision: 512 MB ‚Äì 1 GB Redis instance

6. PostgreSQL Storage Estimation
Ball Events

Events per match: 350

Matches per day: 100

Events per day: 35,000

Yearly Storage
35,000 √ó 365 ‚âà 12.7M rows

Row Size

~300 bytes / row

12.7M √ó 300B ‚âà 3.8 GB / year


‚û°Ô∏è PostgreSQL easily handles this with indexing.

7. API Throughput
REST APIs

Peak: ~2K RPS

FastAPI can handle ~5‚Äì10K RPS per instance

‚û°Ô∏è 5‚Äì6 API instances sufficient

8. Scaling Strategy
8.1 Horizontal Scaling
Component	Strategy
API	Stateless, autoscale
WebSocket	Redis Pub/Sub
Redis	Primary + replica
DB	Read replicas
8.2 Autoscaling Triggers

CPU > 70%

Active WebSocket connections

Redis memory usage

API response latency

9. Fault Tolerance
Failure	Handling
API crash	Load balancer reroutes
WS server down	Client reconnect
Redis down	Read from DB
DB down	Read-only degraded mode
10. Cost Awareness (Rough)
Component	Estimate
API instances	Medium
Redis	Low
DB	Medium
Bandwidth	High (WebSockets)

‚û°Ô∏è WebSocket traffic is the dominant cost

11. Bottlenecks & Mitigation
Bottleneck	Mitigation
WS fan-out	Horizontal scale
Redis Pub/Sub	Sharding by match
DB writes	Async batching
Cold starts	Warm pools
12. Trade-offs
Decision	Reason
WebSockets	True real-time
Redis	Low latency
Eventual consistency	Performance
13. Final Summary

Write load is minimal

Read & fan-out dominate

Redis is critical

WebSockets define scale

Architecture supports 10√ó growth